# 决策树



## 1. 基本概念

以一个二分类的问题为例，在一个给定的训练数据集中，选取一个属性，判断该属性的"是"与"不是"，接着在已判断的基础上继续对其他属性判断，依次递归，直到得到最后的决策，整个过程就形成一棵决策树。



一般的，一棵鞠策树包含一个更节点、若干个内部节点和若干个叶节点；叶节点对应决策结果，其他每个节点则对应一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集。

决策树生成的基本流程：

![image-20210602105743973](https://kinvy-images.oss-cn-beijing.aliyuncs.com/Images/image-20210602105743973.png)



> 决策树的构建：特征选择，决策树树生成，决策树的修剪



## 2. 决策树的构造



在构造决策树时需要解决的第一个问题就是，如何选择最优划分的属性，即当前数据集上啊哪个特征在划分数据分类时起决定性作用。一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高。



### 2.1 信息增益

**信息熵** 是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$ 中第 $k$ 样本所占的比例为 $p_k(k=1,2,\dots,|\gamma|)$ ,则 $D$ 的信息熵定义为
$$
Ent(D)=-\sum_{k=1}^{|\gamma|}p_k\log_2p_k
$$
$Ent(D)$ 的值越小，则 $D$ 的纯度越高。

假定离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1,a^2,\dots,a^V\}$ ,若使用 $a$ 来对样本集 $D$ 进行划分，则会产生  $V$ 个分支点，其中第 $v$ 个分支节点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$ .我们可根据式 $(1)$ 计算出 $D^v$ 的信息熵，再考虑到不同的分支点所包含的样本树不同，给分支节点赋予权重 $|D^v|/|D|$ ,即样本数越多的分支点的影响越大，于是可计算出用属性 $a$ 对样本集 $D$ 进行划分所获得的"信息增益"
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^{v}\frac{|D^v|}{|D|}Ent(D^v)
$$
一般而言，信息增益越大，则一维这使用属性 $a$ 来划分所获得的"纯度提升"越大。 $ID3$ 决策树学习算法就是以信息增益为准则来选择划分属性。

以下表中的数据为例，使用信息增益构建决策树

![image-20210602204905436](https://kinvy-images.oss-cn-beijing.aliyuncs.com/Images/image-20210602204905436.png)



计算各个属性的信息增益：

$Gain(D,色泽)=0.109$ ;	$Gain(D,根蒂)=0.143$;	$Gain(D,敲声)=0.141$;	$Gain(D,纹理)=0.381$;

$Gain(D,脐部)=0.289$;	$Gain(D,触感)=0.006$

显然，属性“纹理”的信息增益最大，于是它选为划分属性。基于纹理对根节点进行划分

![image-20210604131351224](https://kinvy-images.oss-cn-beijing.aliyuncs.com/Images/image-20210604131351224.png)

然后，决策树学习算法将对每个分支节点做进一步划分。





### 2.2 增益率

实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏号可能带来的不利影响，

$C4.5$ 决策树算法不直接使用信息增益，而是使用“增益率”（gain ratio)来选择最优划分属性，增益率的定义为：
$$
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a)=-\sum_{v = 1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{D}
$$
称为属性 $a$ 的“固有值”.属性$a$ 的可能取值数目越多（即 $V$ 越大），则 $IV(a)$ 的值通常会越大。

需要注意的是，增益率则对可取值数目较少的属性有所偏好，因此， $C4.5$ 算法并不是直接选择增益率最大的时候划分属性，而是使用一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。



### 2.3  基尼指数

CART 决策树使用“基尼指数”（Gini index)来选择划分属性，采用 $(1)$ 相同的符号，数据集 $D$ 的纯度可以基尼值来度量:
$$
Gini(D)= \sum_{k=1}^{|\gamma|}\sum_{k'\neq k}p_kp_{k'}\\
=1-\sum_{k=1}^{|\gamma|}p_k^2
$$
$Gini(D)$ 反映了从数据 $D$ 中随机抽取两个样本，其类别标记不一致的概率。因此， $Gini(D)$ 越小，则数据集 $D$ 的纯度越高

采用与式 $(2)$ 相同的符号表示，属性 $a$ 的基尼指数定义为：
$$
Gini\_index(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)
$$
于是，我们在候选属性集合A中，选择那个使得划分和基尼指数最小属性作为最优划分属性，即

$a_*=\underset{a\in A}{arg\ min}\ Gini\_index(D,a)$







## 3.  剪枝处理

剪枝是决策学习算法对付过拟合的主要手段，决策树剪枝的基本策略有**预剪枝**和**后剪枝**。

预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对于的子树代替为也节点能带来决策树泛化性能提升，则将该子树替换为也节点。



*预剪枝和后剪枝实例说明后续补充*



## 4. 连续与缺失值



### 4.1 连续值处理

由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分，使用连续属性离散化技术就可。最简单的策略是是采用二分法对连续属性进行离散处理，这正是 $C4.5$ 决策树算法中采用的机制。

给定样本集合 $D$ 和连续属性 $a$ ,假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值，将这些值从小到大进行排序，记为 $\{a^1,a^2,\dots,a^n\}$. 基于划分点 $t$ 可将 $D$ 分为子集 $D_t^-$ 和 $D_t^+$,其中 $D_t^-$ 包含那些在属性 $a$上取值不大于 $t$ 的样本，而 $D_t^+$ 则是大于 $t$ 的样本。显然 ，对相邻的属性取值 $a^i$ 与 $a^{i+1}$ 来说， $t$ 在区间$[a^i,a^{i+1})$ 中取任意值所产生的划分结果相同，因此，对连续属性 $a$ ,我们可考察包含 $n-1$ 个元素的候选划分集合
$$
T_a=\{\frac{a^i+a^{i+1}}{2}|1\leq i \leq n-1\}
$$
即吧区间 $[a^i,a^{i+1})$ 的中位点 $\frac{a^i+a^{i+1}}{2}$ 作为候选划分点。然后，我们就可像离散属性值一样来考察这些划分点，选取最优的划分定进行样本集合的划分吗，例如，对式 $(2)$ 稍加改造:
$$
Gain(D,a)=\underset{t\in T_a}{max}\ Gain(D,a,t)\\
=\underset{t\in T_a}{max}\ Ent(D)-\sum_{\lambda\in\{-,+\}}\frac{|D_{t}^{\lambda}|}{|D|}Ent(D_t^{\lambda})
$$
其中 $Gain(D,a,t)$ 是样本集 $D$ 基于划分点 $t$ 二分后的信息增益，于是，我们就选择是 $Gain(D,a,t)$ 最大化的划分点。



*示例后续补充*



### 4.2 缺失值处理

（TODO）



## 5. 多变量决策树









