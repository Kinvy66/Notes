# 决策树



## 1. 基本概念

以一个二分类的问题为例，在一个给定的训练数据集中，选取一个属性，判断该属性的"是"与"不是"，接着在已判断的基础上继续对其他属性判断，依次递归，直到得到最后的决策，整个过程就形成一棵决策树。



一般的，一棵鞠策树包含一个更节点、若干个内部节点和若干个叶节点；叶节点对应决策结果，其他每个节点则对应一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集。

决策树生成的基本流程：

![image-20210602105743973](https://kinvy-images.oss-cn-beijing.aliyuncs.com/Images/image-20210602105743973.png)



> 决策树的构建：特征选择，决策树树生成，决策树的修剪



## 2. 决策树的构造



在构造决策树时需要解决的第一个问题就是，如何选择最优划分的属性，即当前数据集上啊哪个特征在划分数据分类时起决定性作用。一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高。



### 2.1 信息增益

**信息熵** 是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$ 中第 $k$ 样本所占的比例为 $p_k(k=1,2,\dots,|\gamma|)$ ,则 $D$ 的信息熵定义为
$$
Ent(D)=-\sum_{k=1}^{|\gamma|}p_k\log_2p_k
$$
$Ent(D)$ 的值越小，则 $D$ 的纯度越高。

假定离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1,a^2,\dots,a^V\}$ ,若使用 $a$ 来对样本集 $D$ 进行划分，则会产生  $V$ 个分支点，其中第 $v$ 个分支节点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$ .我们可根据式 $(1)$ 计算出 $D^v$ 的信息熵，再考虑到不同的分支点所包含的样本树不同，给分支节点赋予权重 $|D^v|/|D|$ ,即样本数越多的分支点的影响越大，于是可计算出用属性 $a$ 对样本集 $D$ 进行划分所获得的"信息增益"
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^{v}\frac{|D^v|}{|D|}Ent(D^v)
$$
一般而言，信息增益越大，则一维这使用属性 $a$ 来划分所获得的"纯度提升"越大。 $ID3$ 决策树学习算法就是以信息增益为准则来选择划分属性。

以下表中的数据为例，使用信息增益构建决策树

![image-20210602204905436](https://kinvy-images.oss-cn-beijing.aliyuncs.com/Images/image-20210602204905436.png)



计算各个属性的信息增益：

$Gain(D,色泽)=0.109$ ;	$Gain(D,根蒂)=0.143$;	$Gain(D,敲声)=0.141$;	$Gain(D,纹理)=0.381$;

$Gain(D,脐部)=0.289$;	$Gain(D,触感)=0.006$

显然，属性“纹理”的信息增益最大，于是它选为划分属性。基于纹理对根节点进行划分

![image-20210604131351224](https://kinvy-images.oss-cn-beijing.aliyuncs.com/Images/image-20210604131351224.png)

