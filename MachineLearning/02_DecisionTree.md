# 决策树



## 1. 基本概念

以一个二分类的问题为例，在一个给定的训练数据集中，选取一个属性，判断该属性的"是"与"不是"，接着在已判断的基础上继续对其他属性判断，依次递归，直到得到最后的决策，整个过程就形成一棵决策树。



一般的，一棵鞠策树包含一个更节点、若干个内部节点和若干个叶节点；叶节点对应决策结果，其他每个节点则对应一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集。

决策树生成的基本流程：

![image-20210602105743973](https://kinvy-images.oss-cn-beijing.aliyuncs.com/Images/image-20210602105743973.png)



> 决策树的构建：特征选择，决策树树生成，决策树的修剪



## 2. 决策树的构造



在构造决策树时需要解决的第一个问题就是，如何选择最优划分的属性，即当前数据集上啊哪个特征在划分数据分类时起决定性作用。一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高。



### 2.1 信息增益

**信息熵** 是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$ 中第 $k$ 样本所占的比例为 $p_k(k=1,2,\dots,|\gamma|)$ ,则 $D$ 的信息熵定义为
$$
Ent(D)=-\sum_{k=1}^{|\gamma|}p_k\log_2p_k
$$
$Ent(D)$ 的值越小，则 $D$ 的纯度越高。

